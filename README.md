# ğŸ§  llm-router

`llm-router` is a backend application built with FastAPI that allows you to route API requests to different Large Language Models (LLMs) like OpenAI and Gemini, and optionally enhance responses using Retrieval-Augmented Generation (RAG).

---

## ğŸš€ Features

- ğŸ” Route prompts dynamically to OpenAI or Gemini based on input
- ğŸ“„ RAG support with `.txt` and `.pdf` files
- ğŸ§  Uses `sentence-transformers` (`all-MiniLM-L6-v2`) for embedding
- ğŸ“‚ Upload files through `/upload`, delete via `/clear-data`
- ğŸ“¡ Simple `/ask` endpoint to query LLMs with or without RAG

---

## ğŸ“ Project Structure

```
app/
â”œâ”€â”€ main.py                 # FastAPI app entrypoint
â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ prompt.py           # /ask endpoint
â”‚   â””â”€â”€ files.py            # /upload and /clear-data
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ model_router.py     # routes requests to correct LLM client
â”‚   â”œâ”€â”€ file_operations.py  # file save/delete logic
â”‚   â””â”€â”€ rag_engine.py       # RAG embedding & prompt context
â”œâ”€â”€ data/                   # Holds uploaded .txt and .pdf files
```

---

## ğŸ“¦ Requirements

- Python 3.10+
- `fastapi`, `uvicorn`, `python-multipart`
- `openai`, `google-genai`
- `sentence-transformers`, `PyMuPDF`

---

## ğŸ”‘ Environment Variables

Create a `.env` file in the root directory:

```
HOST=your_host_name
PORT=your_port
OPENAI_API_KEY=your-openai-key
GEMINI_API_KEY=your-gemini-key
```

---

## ğŸ“¡ API Endpoints

### `/ask` (POST)
Send prompt to selected LLM.

```json
{
  "prompt": "Explain transformers",
  "client": "openai",            // or "gemini"
  "model": "any openai model",              // or "gemini model"
  "use_rag": true
}
```

Returns:
```json
{
  "response": "...",
  "model_used": "selected model",
  "rag_used": true // or false if "use_rag" option was opted out or if something went wrong
}
```

---

### `/upload` (POST)
Upload `.txt` or `.pdf` file to use for RAG.

- Form-data key: `file`
- Example: use Postman or curl

```bash
curl -F "file=@yourdoc.pdf" localhost/upload
```

---

### `/clear-data` (DELETE)
Deletes all uploaded files in `app/data/`.

```bash
curl -X DELETE localhost/clear-data
```

---

## ğŸ Initialize Project Locally

```bash
$ git clone https://github.com/r00kieAd/llm-router.git
$ python -m venv venv
$ pip install -r requirements
$ python main.py # for custom host and port using .env
$ uvicorn main:app # will give default uri http://127.0.0.1:8000
```

## ğŸ§ª Testing

- Test LLMs with and without RAG using Postman or frontend
- Upload `.txt`/`.pdf` before enabling `use_rag: true`
- Validate context injection by comparing answers

---

## ğŸ› ï¸ To Do (Next Steps)

- [ ] Add support for `.md`, `.docx`, `.csv`
- [ ] Implement file-specific RAG selection
- [ ] Deploy to cloud (Render, Vercel, AWS)

---

## ğŸ‘¤ Author

Developed by [@adhyatmadwivedi](https://github.com/r00kieAd) â€“ for personal assistant & AI tooling use-cases.

---
