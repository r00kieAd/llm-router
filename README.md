### ğŸ§  llm-router

`llm-router` is a backend application built with FastAPI that allows you to route API requests to different Large Language Models (LLMs) like OpenAI and Gemini, and optionally enhance responses using Retrieval-Augmented Generation (RAG).

# "work is in progress so below information is not up to date with the repository"

---

## ğŸš€ Features

- ğŸ” Route prompts dynamically to OpenAI or Gemini based on input
- ğŸ“„ RAG support with `.txt` and `.pdf` files
- ğŸ§  Uses `sentence-transformers` (`all-MiniLM-L6-v2`) for embedding
- ğŸ“‚ Upload files through `/upload`, delete via `/clear-data`
- ğŸ“¡ Simple `/ask` endpoint to query LLMs with or without RAG

---

## ğŸ“ Project Structure

```
app/
â”œâ”€â”€ main.py                 # FastAPI app entrypoint
â”œâ”€â”€ data/                # Folder for storing Temporary data to be used by RAG and LLM
â”œâ”€â”€ rag
|   â”œâ”€â”€ embedder.py         # to embed the documents (if available) and prompt
â”‚   â”œâ”€â”€ reriever.py         # to retrieve the embeddings and make it as numeric array using numpy
â”‚   â””â”€â”€ rag_engine.py       # RAG embedding & prompt context
â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ prompt.py           # /ask endpoint
â”‚   â””â”€â”€ files.py            # /upload and /clear-data are routed accordingly to file_operaitons.py
|   â””â”€â”€ start.py            # app run confirm
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ gemini_client.py    # sends generated prompt to gemini and returns the response
â”‚   â”œâ”€â”€ openai_client.py    # sends generated prompt to openai and returns the response
â”‚   â””â”€â”€ model_router.py     # route service to ping the correct llm client as per request
|   â””â”€â”€ file_operations.py  # all operations involving upload and delete
```

---

## ğŸ“¦ Requirements

- Python 3.10+
- `fastapi`, `uvicorn`, `python-multipart`
- `openai`, `google-genai`
- `sentence-transformers`, `PyMuPDF`

Install dependencies:
```bash
pip install -r requirements.txt
```

---

## ğŸ”‘ Environment Variables

Create a `.env` file in the root directory:

```
OPENAI_API_KEY=your-openai-key
GEMINI_API_KEY=your-gemini-key
```

---

## ğŸ“¡ API Endpoints

### `/ask` (POST)
Send prompt to selected LLM.

```json
{
  "prompt": "Explain transformers",
  "client": "openai",            // or "gemini"
  "model": "any openai model",              // or "gemini model"
  "use_rag": true
}
```

Returns:
```json
{
  "response": "...",
  "model_used": "selected model",
  "rag_used": true // or false if "use_rag" option was opted out or if something went wrong
}
```

---

### `/upload` (POST)
Upload `.txt` or `.pdf` file to use for RAG.

- Form-data key: `file`
- Example: use Postman or curl

```bash
curl -F "file=@yourdoc.pdf" localhost/upload
```

---

### `/clear-data` (DELETE)
Deletes all uploaded files in `app/data/`.

```bash
curl -X DELETE localhost/clear-data
```

---

## ğŸ§ª Testing

- Test LLMs with and without RAG using Postman or frontend
- Upload `.txt`/`.pdf` before enabling `use_rag: true`
- Validate context injection by comparing answers

---

## ğŸ› ï¸ To Do (Next Steps)

- [ ] Add support for `.md`, `.docx`, `.csv`
- [ ] Implement file-specific RAG selection
- [ ] Deploy to cloud (Render, Vercel, AWS)

---

## ğŸ‘¤ Author

Developed by [@adhyatmadwivedi](https://github.com/r00kieAd) â€“ for personal assistant & AI tooling use-cases.

---
